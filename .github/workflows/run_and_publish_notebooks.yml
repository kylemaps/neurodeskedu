# =============================================================================
# Run Notebooks & Publish to GitHub Pages
# =============================================================================
# This workflow:
#   1. Detects new or modified Jupyter notebooks
#   2. Executes them in a Neurodesk container to validate they run correctly
#   3. Builds the Jupyter Book and publishes to GitHub Pages
#
# Triggers:
#   - Push to main (when notebooks or book markdown files change)
#   - Pull requests to main (for validation, no deployment)
#   - Manual dispatch (with options for image override, force execution, etc.)
# =============================================================================

name: Run Notebooks & Publish

defaults:
  run:
    shell: bash

# =============================================================================
# TRIGGERS
# =============================================================================
on:
  push:
    branches: [main]
    paths:
      - '**/*.ipynb'
      - 'books/**/*.md'

  pull_request:
    branches: [main]
    paths:
      - '**/*.ipynb'
      - 'books/**/*.md'
    types: [opened, synchronize, reopened, ready_for_review]

  workflow_dispatch:
    inputs:
      image_override:
        description: 'Override the Docker image (e.g. ghcr.io/neurodesk/neurodesktop/neurodesktop:2024-01-01)'
        required: false
        type: string
      force_execution:
        description: 'Force execution of all notebooks'
        required: false
        type: boolean
        default: false
      force_deploy_pages:
        description: 'Force deploy pages'
        required: false
        type: boolean
        default: false
      notebook_selector:
        description: 'Path relative to books/ (e.g. examples/workflows/nipype_short.ipynb)'
        required: false
        type: string
      use_self_hosted:
        description: 'Use self-hosted runner (check for self-hosted, default: GitHub-hosted)'
        required: false
        type: boolean
        default: false

# =============================================================================
# CONCURRENCY
# =============================================================================
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

# =============================================================================
# JOBS
# =============================================================================
jobs:

  # ---------------------------------------------------------------------------
  # SETUP: Determine runner and Neurodesk image
  # ---------------------------------------------------------------------------
  setup:
    name: Setup Environment
    runs-on: ubuntu-22.04
    outputs:
      runner: ${{ steps.select_runner.outputs.runner }}
      neurodesk_image: ${{ steps.get_image.outputs.neurodesk_image }}

    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Select runner
        id: select_runner
        run: |
          # For workflow_dispatch, use the use_self_hosted input
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            if [ "${{ inputs.use_self_hosted }}" = "true" ]; then
              echo "runner=self-hosted" >> $GITHUB_OUTPUT
            else
              echo "runner=ubuntu-22.04" >> $GITHUB_OUTPUT
            fi
          # For push/PR, use self-hosted for main repo, ubuntu for forks
          elif [ "${{ github.repository }}" = "neurodesk/neurodeskedu" ]; then
            echo "runner=self-hosted" >> $GITHUB_OUTPUT
          else
            echo "runner=ubuntu-22.04" >> $GITHUB_OUTPUT
          fi

      - name: Determine Neurodesk image version
        id: get_image
        run: |
          if [ -n "${{ inputs.image_override }}" ]; then
            echo "neurodesk_image=${{ inputs.image_override }}" >> $GITHUB_OUTPUT
          else
            curl -s https://raw.githubusercontent.com/neurodesk/neurodesk.github.io/main/data/neurodesktop.toml -o neurodesktop.toml
            VERSION=$(grep 'jupyter_neurodesk_version = ' neurodesktop.toml | cut -d '"' -f 2)
            echo "neurodesk_image=ghcr.io/neurodesk/neurodesktop/neurodesktop:$VERSION" >> $GITHUB_OUTPUT
          fi

      - name: Cache Docker image
        id: docker-cache
        uses: actions/cache@v5.0.1
        with:
          path: ~/docker-image-cache
          key: docker-neurodesk-${{ steps.get_image.outputs.neurodesk_image }}

      - name: Prefetch Docker image
        if: steps.docker-cache.outputs.cache-hit != 'true'
        run: |
          IMAGE="${{ steps.get_image.outputs.neurodesk_image }}"
          CACHE_FILE=~/docker-image-cache/neurodesk.tar
          mkdir -p ~/docker-image-cache
          
          echo "Cache miss. Pulling $IMAGE..."
          docker pull "$IMAGE"
          
          echo "Saving image to cache..."
          docker save -o "$CACHE_FILE" "$IMAGE"

      # --- CVMFS Cache Warmer ---
      - name: Cache CVMFS apt packages
        uses: actions/cache@v5.0.1
        with:
          path: ${{ github.workspace }}/.cache/cvmfs-apt
          key: cvmfs-apt-${{ runner.os }}-v1

      - name: Setup CVMFS (Warm Cache)
        uses: cvmfs-contrib/github-action-cvmfs@v5.3
        with:
          cvmfs_repositories: neurodesk.ardc.edu.au
          cvmfs_http_proxy: DIRECT
          cvmfs_quota_limit: 5000
          apt_cache: ${{ github.workspace }}/.cache/cvmfs-apt

      # --- Pip Cache Warmer ---
      - name: Cache Conda/Python dependencies
        uses: actions/cache@v5.0.1
        with:
          path: ${{ github.workspace }}/.cache/pip
          key: pip-book-${{ hashFiles('books/_config.yml') }}

      - name: Install Dependencies (Warm Cache)
        env:
          PIP_CACHE_DIR: ${{ github.workspace }}/.cache/pip
        run: |
          mkdir -p ${{ github.workspace }}/.cache/pip
          pip install "jupyter-book<2.0.0" ghp-import

  # ---------------------------------------------------------------------------
  # SELECT NOTEBOOKS: Determine which notebooks to run (changed or manually selected)
  # ---------------------------------------------------------------------------
  select-notebooks:
    name: Select Notebooks to Run
    runs-on: ubuntu-22.04
    outputs:
      notebook_list: ${{ steps.list_notebooks.outputs.notebook_list }}

    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Find changed notebook files
        id: find_changed
        uses: tj-actions/changed-files@c3a1bb2c992d77180ae65be6ae6c166cf40f857c
        with:
          path: "./books"
          files: |
            **/*.ipynb
          dir_names_exclude_current_dir: "true"

      - name: Build notebook list for matrix
        id: list_notebooks
        run: |
          # Determine which notebooks to run based on trigger type
          if [ "${{ inputs.force_execution }}" == "true" ]; then
            echo "Mode: Force execution of ALL notebooks"
            changed_notebooks=$(find books -name "*.ipynb" | sed 's|^books/||')

          elif [ -n "${{ inputs.notebook_selector }}" ]; then
            echo "Mode: Running specific notebook: ${{ inputs.notebook_selector }}"
            changed_notebooks="${{ inputs.notebook_selector }}"

          else
            echo "Mode: Running changed notebooks only"
            echo "Changed files: ${{ steps.find_changed.outputs.all_changed_files }}"
            changed_notebooks=$(echo "${{ steps.find_changed.outputs.all_changed_files }}")
          fi

          # Build JSON array for matrix strategy
          notebook_list='['
          for NOTEBOOK in $(echo "${changed_notebooks}"); do
            notebook_list+="\"${NOTEBOOK}\","
          done
          notebook_list=$(sed '$s/,$//' <<< $notebook_list)
          notebook_list+=']'

          echo "notebook_list=${notebook_list}"
          echo "notebook_list=${notebook_list}" >> $GITHUB_OUTPUT

  # ---------------------------------------------------------------------------
  # RUN NOTEBOOKS: Execute each changed notebook in Neurodesk container
  # ---------------------------------------------------------------------------
  run-notebooks:
    name: Run Notebook
    needs: [setup, select-notebooks]
    if: ${{ needs.select-notebooks.outputs.notebook_list != '[]' }}
    runs-on: ${{ needs.setup.outputs.runner }}
    timeout-minutes: 1500

    strategy:
      fail-fast: false
      matrix:
        notebooks: ${{ fromJson(needs.select-notebooks.outputs.notebook_list) }}

    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Set notebook environment variables
        run: echo "NOTEBOOK=$(basename -s .ipynb ${{ matrix.notebooks }})" >> $GITHUB_ENV

      - name: Configure runner (Optimize Disk Space)
        if: runner.os == 'Linux'
        run: |
          if [ "${{ needs.setup.outputs.runner }}" != "self-hosted" ]; then
             echo "Optimizing GitHub-hosted runner..."
             sudo swapoff -a && sudo rm -rf /mnt/*
             BASE_PATH=/mnt
          else
             echo "Configuring self-hosted runner..."
             # Be careful not to wipe /mnt on self-hosted if it's used
             BASE_PATH=/storage
          fi
          
          echo "Using BASE_PATH=$BASE_PATH"
          echo "BASE_PATH=$BASE_PATH" >> $GITHUB_ENV
          
          # Setup directories
          sudo mkdir -p $BASE_PATH/tmp
          sudo chown $USER $BASE_PATH/tmp
          sudo mkdir -p $BASE_PATH/docker
          
          # Configure Docker to use new data-root
          echo "{\"data-root\": \"$BASE_PATH/docker\"}" | sudo tee /etc/docker/daemon.json
          sudo systemctl restart docker
          
          # Clean up existing space
          docker system prune -af || true

      # -------------------------------------------------------------------------
      # CVMFS Setup (for GitHub-hosted runners)
      # -------------------------------------------------------------------------
      - name: Restore CVMFS apt cache
        uses: actions/cache@v5.0.1
        id: cvmfs-cache
        with:
          path: ${{ github.workspace }}/.cache/cvmfs-apt
          key: cvmfs-apt-${{ runner.os }}-v1

      - name: Create Neurodesk CVMFS public key
        run: |
          sudo mkdir -p /etc/cvmfs/keys/ardc.edu.au
          sudo tee /etc/cvmfs/keys/ardc.edu.au/neurodesk.ardc.edu.au.pub << 'EOF'
          -----BEGIN PUBLIC KEY-----
          MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwUPEmxDp217SAtZxaBep
          Bi2TQcLoh5AJ//HSIz68ypjOGFjwExGlHb95Frhu1SpcH5OASbV+jJ60oEBLi3sD
          qA6rGYt9kVi90lWvEjQnhBkPb0uWcp1gNqQAUocybCzHvoiG3fUzAe259CrK09qR
          pX8sZhgK3eHlfx4ycyMiIQeg66AHlgVCJ2fKa6fl1vnh6adJEPULmn6vZnevvUke
          I6U1VcYTKm5dPMrOlY/fGimKlyWvivzVv1laa5TAR2Dt4CfdQncOz+rkXmWjLjkD
          87WMiTgtKybsmMLb2yCGSgLSArlSWhbMA0MaZSzAwE9PJKCCMvTANo5644zc8jBe
          NQIDAQAB
          -----END PUBLIC KEY-----
          EOF

      - name: Setup CVMFS
        uses: cvmfs-contrib/github-action-cvmfs@v5.3
        with:
          cvmfs_repositories: neurodesk.ardc.edu.au
          cvmfs_http_proxy: DIRECT
          cvmfs_quota_limit: 5000
          cvmfs_keys_dir: /etc/cvmfs/keys/ardc.edu.au
          cvmfs_server_url: "http://cvmfs-geoproximity.neurodesk.org/cvmfs/@fqrn@;http://cvmfs.neurodesk.org/cvmfs/@fqrn@;http://s1osggoc-cvmfs.openhtc.io:8080/cvmfs/@fqrn@;http://s1fnal-cvmfs.openhtc.io:8080/cvmfs/@fqrn@;http://s1sampa-cvmfs.openhtc.io:8080/cvmfs/@fqrn@;http://s1brisbane-cvmfs.openhtc.io/cvmfs/@fqrn@;http://s1nikhef-cvmfs.openhtc.io/cvmfs/@fqrn@;http://s1bnl-cvmfs.openhtc.io/cvmfs/@fqrn@;http://s1perth-cvmfs.openhtc.io/cvmfs/@fqrn@"
          apt_cache: ${{ github.workspace }}/.cache/cvmfs-apt

      - name: Mount CVMFS
        run: |
          # Probe and verify mount
          sudo cvmfs_config probe neurodesk.ardc.edu.au || true
          
          # Trigger autofs mount
          ls /cvmfs/neurodesk.ardc.edu.au > /dev/null 2>&1 || {
            echo "Autofs mount failed, attempting manual mount..."
            sudo mkdir -p /cvmfs/neurodesk.ardc.edu.au
            sudo mount -t cvmfs neurodesk.ardc.edu.au /cvmfs/neurodesk.ardc.edu.au
          }
          
          # Verify
          echo "CVMFS mounted:"
          ls /cvmfs/neurodesk.ardc.edu.au/ | head -5

      # -------------------------------------------------------------------------
      # Docker image caching
      # -------------------------------------------------------------------------
      - name: Restore Docker image cache
        uses: actions/cache@v5.0.1
        id: docker-cache
        with:
          path: ~/docker-image-cache
          key: docker-neurodesk-${{ needs.setup.outputs.neurodesk_image }}

      - name: Load Neurodesk image
        run: |
          IMAGE="${{ needs.setup.outputs.neurodesk_image }}"
          CACHE_FILE=~/docker-image-cache/neurodesk.tar
          
          if [ -f "$CACHE_FILE" ]; then
            echo "Loading image from cache..."
            docker load -i "$CACHE_FILE"
          else
            echo "Cache not found (unexpected). Pulling image..."
            docker pull "$IMAGE"
          fi
          
          echo "Image ready:"
          docker images "$IMAGE"

      # -------------------------------------------------------------------------
      # Docker execution with CVMFS mounted
      # -------------------------------------------------------------------------

      - name: Fix notebook kernelspec if needed
        run: |
          python3 << 'EOF'
          import json
          import sys

          notebook_path = "books/${{ matrix.notebooks }}"
          try:
              with open(notebook_path, 'r') as f:
                  notebook = json.load(f)

              # Fix conda-base-py or other problematic kernels
              if 'metadata' in notebook and 'kernelspec' in notebook['metadata']:
                  kernel_name = notebook['metadata']['kernelspec'].get('name', '')
                  if kernel_name == 'conda-base-py' or 'conda' in kernel_name:
                      print(f"Fixing kernelspec in {notebook_path}: {kernel_name} -> python3")
                      notebook['metadata']['kernelspec'] = {
                          "display_name": "Python 3",
                          "language": "python",
                          "name": "python3"
                      }
                      with open(notebook_path, 'w') as f:
                          json.dump(notebook, f, indent=1, ensure_ascii=False)
                          f.write('\n')
                  else:
                      print(f"Kernelspec OK: {kernel_name}")
          except Exception as e:
              print(f"Note: Could not process notebook (may not be ipynb): {e}")
          EOF

      - name: Set notebook environment variables
        run: |
          echo "NOTEBOOK=$(basename -s .ipynb ${{ matrix.notebooks }})" >> $GITHUB_ENV
          echo "DIRNAME=$(dirname ${{ matrix.notebooks }})" >> $GITHUB_ENV
          echo "CONTAINER_NAME=neurodesktop-${{ github.run_id }}-${{ strategy.job-index }}" >> $GITHUB_ENV

      - name: Start Neurodesktop container
        env:
          GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
        run: |
          docker run -d --name "$CONTAINER_NAME" \
            --shm-size=1gb \
            --privileged \
            --user=root \
            -v ${{ github.workspace }}:/home/jovyan/workspace \
            -v /cvmfs:/cvmfs:ro \
            -v ${{ env.BASE_PATH }}/tmp:/overlay-storage \
            -v ${{ github.workspace }}/.cache/pip:/home/jovyan/.cache/pip \
            -w /home/jovyan/workspace/books \
            -e NB_USER=jovyan \
            -e NB_UID="$(id -u)" \
            -e NB_GID="$(id -g)" \
            -e GITHUB_PAT="${GITHUB_PAT}" \
            -e NOTEBOOK="${NOTEBOOK}" \
            ${{ needs.setup.outputs.neurodesk_image }}
          
          echo "Started container: $CONTAINER_NAME"

      - name: Wait for container readiness
        run: |
          echo "Waiting for Neurodesktop to become ready..."
          for i in {1..120}; do
            if docker logs "$CONTAINER_NAME" 2>&1 | grep -q "To access the server"; then
              echo "Container ready after ${i} seconds"
              exit 0
            fi
            sleep 1
          done
          echo "::error::Container did not become ready within 120 seconds"
          echo "=== Container logs ==="
          docker logs "$CONTAINER_NAME"
          exit 1

      - name: Execute notebook in container
        timeout-minutes: 1500
        run: |
          docker exec "$CONTAINER_NAME" bash -c '
            set -e
            export PATH=/opt/conda/bin:$HOME/.local/bin:$PATH

            # Initialize the module system
            if [ -f /usr/share/lmod/lmod/init/bash ]; then
              source /usr/share/lmod/lmod/init/bash
            elif [ -f /usr/share/module.sh ]; then
              source /usr/share/module.sh
            fi

            # Create apptainer overlay for modules that require it
            if [ ! -L /tmp/apptainer_overlay ]; then
              echo "Creating /tmp/apptainer_overlay symlink to /overlay-storage..."
              
              # Check if overlay exists in mounted storage
              if [ ! -f /overlay-storage/apptainer_overlay ]; then
                  echo "Creating 300MB overlay file on secondary disk..."
                  dd if=/dev/zero of=/overlay-storage/apptainer_overlay bs=1M count=300 status=none
                  mkfs.ext3 -F /overlay-storage/apptainer_overlay >/dev/null
                  chmod 777 /overlay-storage/apptainer_overlay
              fi
              
              # Symlink to expected location
              ln -sf /overlay-storage/apptainer_overlay /tmp/apptainer_overlay
            fi

            # Move apptainer working dir to secondary storage to save root disk space
            mkdir -p /overlay-storage/var_lib_apptainer
            chmod 777 /overlay-storage/var_lib_apptainer
            
            # Symlink /var/lib/apptainer if it doesnt exist or isnt a link yet
            if [ ! -L /var/lib/apptainer ]; then
                rm -rf /var/lib/apptainer # Remove if it exists as a dir
                ln -sf /overlay-storage/var_lib_apptainer /var/lib/apptainer
            fi

            # Install dependencies (use pip --user to avoid /opt/conda permission issues)
            pip install --user nbconvert nbclient ipykernel

            # Pre-execution verification
            echo "=== Container Environment ==="
            which jupyter && jupyter --version
            echo "Current directory: $(pwd)"
            echo "Notebook to execute: ${{ matrix.notebooks }}"

            echo "Executing notebook: ${{ matrix.notebooks }}"
            
            # Execute the notebook in-place using nbconvert
            # --allow-errors: continue even if cells error (we detect errors later)
            # --inplace: overwrite the original notebook with outputs
            # Log is saved to mounted workspace for persistence
            LOG_FILE="/home/jovyan/workspace/nbconvert-${NOTEBOOK}.log"
            jupyter nbconvert --to notebook --execute --inplace --allow-errors \
              --ExecutePreprocessor.timeout=3600 \
              --ExecutePreprocessor.kernel_name=python3 \
              --debug \
              ${{ matrix.notebooks }} 2>&1 | tee "$LOG_FILE"
            
            echo "Notebook execution complete"
            
            # Post-execution verification
            echo "=== Post-execution check ==="
            if [ -f "${{ matrix.notebooks }}" ]; then
              ls -la "${{ matrix.notebooks }}"
            else
              echo "::error::Notebook file not found after execution: ${{ matrix.notebooks }}"
              exit 1
            fi
          '

      - name: Cleanup container
        if: always()
        run: |
          echo "Stopping and removing container: $CONTAINER_NAME"
          docker stop "$CONTAINER_NAME" 2>/dev/null || true
          docker rm "$CONTAINER_NAME" 2>/dev/null || true

      - name: Check for execution errors
        run: |
          # Check the notebook for errors - step/job fails (red) if errors found, but publish still runs
          python3 << 'EOF'
          import json
          import sys
          import os
          
          notebook_path = "books/${{ matrix.notebooks }}"
          notebook_name = "${{ matrix.notebooks }}"
          errors_found = []
          
          try:
              with open(notebook_path, 'r') as f:
                  nb = json.load(f)
              
              for cell_idx, cell in enumerate(nb.get('cells', [])):
                  if cell.get('cell_type') == 'code':
                      for output in cell.get('outputs', []):
                          if output.get('output_type') == 'error':
                              ename = output.get('ename', 'Unknown')
                              evalue = output.get('evalue', '')
                              errors_found.append({
                                  'cell': cell_idx,
                                  'ename': ename,
                                  'evalue': evalue[:200]  # Truncate long error messages
                              })
              
              if errors_found:
                  # Report as error so the step shows as failed
                  print(f"::error::Notebook {notebook_name} has {len(errors_found)} execution error(s). Please fix the notebook.")
                  
                  # Write to job summary for visibility
                  summary = f"""
          ## ⚠️ Notebook Execution Errors
          
          **Notebook:** `{notebook_name}`
          
          The following errors occurred during execution. The notebook has been published with error outputs visible, but the author should fix these issues:
          
          | Cell | Error Type | Message |
          |------|-----------|---------|
          """
                  for err in errors_found:
                      summary += f"| {err['cell']} | `{err['ename']}` | {err['evalue'][:100]} |\n"
                  
                  summary += "\n\nPlease update the notebook to fix these errors.\n"
                  
                  # Write to GitHub step summary
                  with open(os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null'), 'a') as f:
                      f.write(summary)
                  
                  # Exit with error so step shows as failed (continue-on-error allows workflow to proceed)
                  sys.exit(1)
              else:
                  print("No errors found in notebook outputs")
                  
          except Exception as e:
              print(f"::warning::Could not check notebook for errors: {e}")
          EOF

      - name: Upload executed notebook artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f
        with:
          name: executed-${{ env.NOTEBOOK }}
          path: books/${{ matrix.notebooks }}
          retention-days: 1

  # ---------------------------------------------------------------------------
  # PUBLISH: Build complete book and deploy to GitHub Pages
  # ---------------------------------------------------------------------------
  publish-pages:
    name: Build & Publish to GitHub Pages
    needs: [run-notebooks, setup]
    if: ${{ always() && (needs.run-notebooks.result == 'success' || needs.run-notebooks.result == 'skipped' || needs.run-notebooks.result == 'failure') }}
    runs-on: ubuntu-22.04

    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Configure runner (Optimize Disk Space)
        if: runner.os == 'Linux'
        run: |
          if [ "${{ needs.setup.outputs.runner }}" != "self-hosted" ]; then
             echo "Optimizing GitHub-hosted runner..."
             sudo swapoff -a && sudo rm -rf /mnt/*
             BASE_PATH=/mnt
          else
             echo "Configuring self-hosted runner..."
             # Be careful not to wipe /mnt on self-hosted if it's used
             BASE_PATH=/storage
          fi
          
          echo "Using BASE_PATH=$BASE_PATH"
          echo "BASE_PATH=$BASE_PATH" >> $GITHUB_ENV
          
          # Setup directories
          sudo mkdir -p $BASE_PATH/tmp
          sudo chown $USER $BASE_PATH/tmp
          sudo mkdir -p $BASE_PATH/docker
          
          # Configure Docker to use new data-root
          echo "{\"data-root\": \"$BASE_PATH/docker\"}" | sudo tee /etc/docker/daemon.json
          sudo systemctl restart docker
          
          # Clean up existing space
          docker system prune -af || true

      - name: Restore Docker image cache
        uses: actions/cache@v5.0.1
        with:
          path: ~/docker-image-cache
          key: docker-neurodesk-${{ needs.setup.outputs.neurodesk_image }}

      - name: Load Neurodesk image
        run: |
          IMAGE="${{ needs.setup.outputs.neurodesk_image }}"
          CACHE_FILE=~/docker-image-cache/neurodesk.tar
          if [ -f "$CACHE_FILE" ]; then
            echo "Loading image from cache..."
            docker load -i "$CACHE_FILE"
          else
            echo "::error::Docker image cache not found. This should not happen if run-notebooks succeeded."
            docker pull "$IMAGE"
          fi

      - name: Restore Conda/Python dependencies cache
        uses: actions/cache@v5.0.1
        with:
          path: ${{ github.workspace }}/.cache/pip
          key: pip-book-${{ hashFiles('books/_config.yml') }}
          restore-keys: |
            pip-book-

      - name: Download all executed notebook artifacts
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131
        with:
          pattern: executed-*
          path: loaded-artifacts

      - name: Merge executed notebooks into books directory
        run: |
          echo "========== MERGING EXECUTED NOTEBOOKS =========="
          
          echo "=== Step 1: Check artifact directory ==="
          if [ -d "loaded-artifacts" ]; then
            echo "loaded-artifacts directory exists"
            ls -la loaded-artifacts/
          else
            echo "::error::loaded-artifacts directory not found!"
            exit 1
          fi
          
          echo ""
          echo "=== Step 2: Find notebook files ==="
          echo "Searching for .ipynb files in loaded-artifacts..."
          find loaded-artifacts -name "*.ipynb" -type f
          
          echo ""
          echo "=== Step 3: Merge notebooks ==="
          merged=0
          
          # Find all .ipynb files in loaded-artifacts (handles both direct files and subdirectories)
          while IFS= read -r nb_file; do
            if [ -f "$nb_file" ]; then
              nb_name=$(basename "$nb_file")
              echo "Processing: $nb_file"
              echo "  Notebook: $nb_name ($(du -h "$nb_file" | cut -f1))"
              
              # Find target in books/
              target=$(find books -name "$nb_name" -type f | head -1)
              
              if [ -n "$target" ]; then
                echo "  Target: $target"
                echo "  Before: $(du -h "$target" | cut -f1)"
                cp -v "$nb_file" "$target"
                echo "  After: $(du -h "$target" | cut -f1)"
                merged=$((merged + 1))
              else
                echo "  ::warning::No target found for $nb_name in books/"
              fi
              echo ""
            fi
          done < <(find loaded-artifacts -name "*.ipynb" -type f)
          
          echo ""
          echo "=== Step 4: Verify merge ==="
          echo "Notebooks merged: $merged"
          if [ $merged -eq 0 ]; then
            echo "::warning::No notebooks were merged!"
          fi
          echo ""
          echo "Recently modified .ipynb files in books/:"
          find books -name "*.ipynb" -type f -mmin -5 -exec ls -lh {} \; | head -10
          
          echo "================================================"

      - name: Set container name
        run: echo "CONTAINER_NAME=neurodesktop-publish-${{ github.run_id }}" >> $GITHUB_ENV

      - name: Start Neurodesktop container
        env:
          GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Ensure pip cache directory exists
          mkdir -p ${{ github.workspace }}/.cache/pip

          # -----------------------------------------------------------------------
          # NOTE: We use the exact same Docker startup settings as run-notebooks
          # to ensure the environment (Lmod, Apptainer overlay) initializes
          # correctly without permission errors.
          # -----------------------------------------------------------------------
          docker run -d --name "$CONTAINER_NAME" \
            --shm-size=1gb \
            --privileged \
            --user=root \
            -v ${{ github.workspace }}:/home/jovyan/workspace \
            -v ${{ env.BASE_PATH }}/tmp:/overlay-storage \
            -v ${{ github.workspace }}/.cache/pip:/home/jovyan/.cache/pip \
            -w /home/jovyan/workspace/books \
            -e NB_USER=jovyan \
            -e NB_UID="$(id -u)" \
            -e NB_GID="$(id -g)" \
            -e GITHUB_PAT="${GITHUB_PAT}" \
            ${{ needs.setup.outputs.neurodesk_image }}
          
          echo "Started container: $CONTAINER_NAME"

      - name: Wait for container readiness
        run: |
          echo "Waiting for Neurodesktop to become ready..."
          for i in {1..120}; do
            if docker logs "$CONTAINER_NAME" 2>&1 | grep -q "To access the server"; then
              echo "Container ready after ${i} seconds"
              exit 0
            fi
            sleep 1
          done
          echo "::error::Container did not become ready within 120 seconds"
          echo "=== Container logs ==="
          docker logs "$CONTAINER_NAME"
          exit 1

      - name: Build Jupyter Book in container
        run: |
          docker exec "$CONTAINER_NAME" bash -c '
            set -e
            export PATH=/opt/conda/bin:$HOME/.local/bin:$PATH

            # Install dependencies
            # We use pip --user to take advantage of the mounted .cache/pip
            pip install --user "jupyter-book<2.0.0" ghp-import

            # Generate table of contents
            /bin/bash ../.github/scripts/write-toc-entry.sh

            # Disable notebook execution (already executed in run-notebooks job)
            sed -i "s/execute_notebooks: .*/execute_notebooks: off/" _config.yml
            
            # Build the book
            jupyter-book build .
            
            # Fix permissions of generated files so host user can read them
            chown -R 1001:1001 _build
          '

      - name: Cleanup container
        if: always()
        run: |
          echo "Stopping and removing container: $CONTAINER_NAME"
          docker stop "$CONTAINER_NAME" 2>/dev/null || true
          docker rm "$CONTAINER_NAME" 2>/dev/null || true

      - name: Verify build output
        run: |
          echo "========== VERIFYING BUILD OUTPUT =========="
          
          if [ -d "books/_build/html" ]; then
            echo "✓ books/_build/html exists"
            echo ""
            echo "Build output size:"
            du -sh books/_build/html
            echo ""
            echo "Top-level files:"
            ls -lh books/_build/html/ | head -20
            echo ""
            echo "Recently modified HTML files:"
            find books/_build/html -name "*.html" -type f -mmin -10 | head -10
          else
            echo "::error::books/_build/html directory not found!"
            echo "Listing books/_build:"
            ls -la books/_build/ || echo "books/_build does not exist"
            exit 1
          fi
          
          echo "============================================"

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@4f9cc6602d3f66b9c108549d475ec49e8ef4d45e
        if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./books/_build/html
          keep_files: true
