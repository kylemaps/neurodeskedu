# =============================================================================
# Run Notebooks & Publish to GitHub Pages
# =============================================================================
# This workflow:
#   1. Detects new or modified Jupyter notebooks
#   2. Executes them in a Neurodesk container to validate they run correctly
#   3. Builds the Jupyter Book and publishes to GitHub Pages
#
# Triggers:
#   - Push to main (when notebooks or book markdown files change)
#   - Pull requests to main (for validation, no deployment)
#   - Manual dispatch (with options for image override, force execution, etc.)
# =============================================================================

name: Run Notebooks & Publish

defaults:
  run:
    shell: bash

# =============================================================================
# TRIGGERS
# =============================================================================
on:
  push:
    branches: [main]
    paths:
      - '**/*.ipynb'
      - 'books/**/*.md'

  pull_request:
    branches: [main]
    paths:
      - '**/*.ipynb'
      - 'books/**/*.md'
    types: [opened, synchronize, reopened, ready_for_review]

  workflow_dispatch:
    inputs:
      image_override:
        description: 'Override the Docker image (e.g. ghcr.io/neurodesk/neurodesktop/neurodesktop:2024-01-01)'
        required: false
        type: string
      force_execution:
        description: 'Force execution of all notebooks'
        required: false
        type: boolean
        default: false
      force_deploy_pages:
        description: 'Force deploy pages'
        required: false
        type: boolean
        default: false
      notebook_selector:
        description: 'Path relative to books/ (e.g. examples/workflows/nipype_short.ipynb)'
        required: false
        type: string
      use_self_hosted:
        description: 'Use self-hosted runner (check for self-hosted, default: GitHub-hosted)'
        required: false
        type: boolean
        default: false

# =============================================================================
# CONCURRENCY (disabled to allow multiple runs)
# =============================================================================
# concurrency:
#   group: ${{ github.workflow }}-${{ github.ref }}
#   cancel-in-progress: false

# =============================================================================
# JOBS
# =============================================================================
jobs:

  # ---------------------------------------------------------------------------
  # SETUP: Determine runner and Neurodesk image
  # ---------------------------------------------------------------------------
  setup:
    name: Setup Environment
    runs-on: ubuntu-22.04
    outputs:
      runner: ${{ steps.select_runner.outputs.runner }}
      neurodesk_image: ${{ steps.get_image.outputs.neurodesk_image }}

    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Select runner
        id: select_runner
        run: |
          # For workflow_dispatch, use the use_self_hosted input
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            if [ "${{ inputs.use_self_hosted }}" = "true" ]; then
              echo "runner=self-hosted" >> $GITHUB_OUTPUT
            else
              echo "runner=ubuntu-22.04" >> $GITHUB_OUTPUT
            fi
          # For push/PR, use self-hosted for main repo, ubuntu for forks
          elif [ "${{ github.repository }}" = "neurodesk/neurodeskedu" ]; then
            echo "runner=self-hosted" >> $GITHUB_OUTPUT
          else
            echo "runner=ubuntu-22.04" >> $GITHUB_OUTPUT
          fi

      - name: Determine Neurodesk image version
        id: get_image
        run: |
          if [ -n "${{ inputs.image_override }}" ]; then
            echo "neurodesk_image=${{ inputs.image_override }}" >> $GITHUB_OUTPUT
          else
            curl -s https://raw.githubusercontent.com/neurodesk/neurodesk.github.io/main/data/neurodesktop.toml -o neurodesktop.toml
            VERSION=$(grep 'jupyter_neurodesk_version = ' neurodesktop.toml | cut -d '"' -f 2)
            echo "neurodesk_image=ghcr.io/neurodesk/neurodesktop/neurodesktop:$VERSION" >> $GITHUB_OUTPUT
          fi


      # --- Pip Cache Warmer ---
      - name: Cache Conda/Python dependencies
        uses: actions/cache@v5.0.1
        with:
          path: ${{ github.workspace }}/.cache/pip
          key: pip-book-${{ hashFiles('books/_config.yml') }}

      - name: Install Dependencies (Warm Cache)
        env:
          PIP_CACHE_DIR: ${{ github.workspace }}/.cache/pip
        run: |
          mkdir -p ${{ github.workspace }}/.cache/pip
          pip install "jupyter-book<2.0.0" ghp-import

  # ---------------------------------------------------------------------------
  # SELECT NOTEBOOKS: Determine which notebooks to run (changed or manually selected)
  # ---------------------------------------------------------------------------
  select-notebooks:
    name: Select Notebooks to Run
    runs-on: ubuntu-22.04
    outputs:
      notebook_list: ${{ steps.list_notebooks.outputs.notebook_list }}

    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Find changed notebook files
        id: find_changed
        uses: tj-actions/changed-files@c3a1bb2c992d77180ae65be6ae6c166cf40f857c
        with:
          path: "./books"
          files: |
            **/*.ipynb
          dir_names_exclude_current_dir: "true"

      - name: Build notebook list for matrix
        id: list_notebooks
        run: |
          # Determine which notebooks to run based on trigger type
          if [ "${{ inputs.force_execution }}" == "true" ]; then
            echo "Mode: Force execution of ALL notebooks"
            changed_notebooks=$(find books -name "*.ipynb" | sed 's|^books/||')

          elif [ -n "${{ inputs.notebook_selector }}" ]; then
            echo "Mode: Running specific notebook: ${{ inputs.notebook_selector }}"
            changed_notebooks="${{ inputs.notebook_selector }}"

          else
            echo "Mode: Running changed notebooks only"
            echo "Changed files: ${{ steps.find_changed.outputs.all_changed_files }}"
            changed_notebooks=$(echo "${{ steps.find_changed.outputs.all_changed_files }}")
          fi

          # Build JSON array for matrix strategy
          notebook_list='['
          for NOTEBOOK in $(echo "${changed_notebooks}"); do
            notebook_list+="\"${NOTEBOOK}\","
          done
          notebook_list=$(sed '$s/,$//' <<< $notebook_list)
          notebook_list+=']'

          echo "notebook_list=${notebook_list}"
          echo "notebook_list=${notebook_list}" >> $GITHUB_OUTPUT

  # ---------------------------------------------------------------------------
  # RUN NOTEBOOKS: Execute each changed notebook in Neurodesk container
  # ---------------------------------------------------------------------------
  run-notebooks:
    name: Run Notebook
    needs: [setup, select-notebooks]
    if: ${{ needs.select-notebooks.outputs.notebook_list != '[]' }}
    runs-on: ${{ needs.setup.outputs.runner }}
    timeout-minutes: 1500

    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        notebooks: ${{ fromJson(needs.select-notebooks.outputs.notebook_list) }}

    steps:
      - name: Fix workspace permissions (for self-hosted runners)
        if: always()
        run: |
          # Fix any root-owned files from previous Docker runs
          sudo chown -R $USER:$USER ${{ github.workspace }} 2>/dev/null || true
          # Fix git-annex read-only files (annex sets -r--r--r-- to protect data)
          sudo chmod -R u+w ${{ github.workspace }}/.git/annex 2>/dev/null || true
          find ${{ github.workspace }} -name '.git' -type d -exec sudo chmod -R u+w {}/annex \; 2>/dev/null || true

      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Set notebook environment variables
        run: echo "NOTEBOOK=$(basename -s .ipynb ${{ matrix.notebooks }})" >> $GITHUB_ENV

      - name: Configure runner (Optimize Disk Space)
        if: runner.os == 'Linux'
        run: |
          if [ "${{ needs.setup.outputs.runner }}" != "self-hosted" ]; then
             echo "Optimizing GitHub-hosted runner..."
             sudo swapoff -a && sudo rm -rf /mnt/*
             BASE_PATH=/mnt
          else
             echo "Configuring self-hosted runner..."
             # Be careful not to wipe /mnt on self-hosted if it's used
             BASE_PATH=/storage
          fi
          
          echo "Using BASE_PATH=$BASE_PATH"
          echo "BASE_PATH=$BASE_PATH" >> $GITHUB_ENV
          
          # Setup directories
          sudo mkdir -p $BASE_PATH/tmp
          sudo chown $USER $BASE_PATH/tmp
          
          # Configure Docker data-root (only for GitHub-hosted runners)
          if [ "${{ needs.setup.outputs.runner }}" != "self-hosted" ]; then
            sudo mkdir -p $BASE_PATH/docker
            echo "{\"data-root\": \"$BASE_PATH/docker\"}" | sudo tee /etc/docker/daemon.json
            sudo systemctl restart docker
            docker system prune -af || true
          fi

      # -------------------------------------------------------------------------
      # Ensure Neurodesk image available
      # -------------------------------------------------------------------------
      - name: Ensure Neurodesk image available
        run: |
          IMAGE="${{ needs.setup.outputs.neurodesk_image }}"
          if docker image inspect "$IMAGE" >/dev/null 2>&1; then
            echo "✓ Image already exists locally"
          else
            echo "Pulling $IMAGE..."
            docker pull "$IMAGE"
          fi

      # -------------------------------------------------------------------------
      # Docker execution with CVMFS mounted
      # -------------------------------------------------------------------------

      - name: Fix notebook kernelspec if needed
        run: |
          python3 << 'EOF'
          import json
          import sys

          notebook_path = "books/${{ matrix.notebooks }}"
          try:
              with open(notebook_path, 'r') as f:
                  notebook = json.load(f)

              # Fix conda-base-py or other problematic kernels
              if 'metadata' in notebook and 'kernelspec' in notebook['metadata']:
                  kernel_name = notebook['metadata']['kernelspec'].get('name', '')
                  if kernel_name == 'conda-base-py' or 'conda' in kernel_name:
                      print(f"Fixing kernelspec in {notebook_path}: {kernel_name} -> python3")
                      notebook['metadata']['kernelspec'] = {
                          "display_name": "Python 3",
                          "language": "python",
                          "name": "python3"
                      }
                      with open(notebook_path, 'w') as f:
                          json.dump(notebook, f, indent=1, ensure_ascii=False)
                          f.write('\n')
                  else:
                      print(f"Kernelspec OK: {kernel_name}")
          except Exception as e:
              print(f"Note: Could not process notebook (may not be ipynb): {e}")
          EOF

      - name: Set notebook environment variables
        run: |
          echo "NOTEBOOK=$(basename -s .ipynb ${{ matrix.notebooks }})" >> $GITHUB_ENV
          echo "DIRNAME=$(dirname ${{ matrix.notebooks }})" >> $GITHUB_ENV
          echo "CONTAINER_NAME=neurodesktop-${{ github.run_id }}-${{ strategy.job-index }}" >> $GITHUB_ENV

      - name: Start Neurodesktop container
        env:
          GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
        run: |
          sudo docker run -d --name "$CONTAINER_NAME" \
            --shm-size=1gb \
            --privileged \
            --user=root \
            -v ${{ github.workspace }}:/home/jovyan/workspace \
            -v ${{ env.BASE_PATH }}/tmp:/overlay-storage \
            -v ${{ github.workspace }}/.cache/pip:/home/jovyan/.cache/pip \
            -w /home/jovyan/workspace/books \
            -e NB_UID="$(id -u)" \
            -e NB_GID="$(id -g)" \
            -e GITHUB_PAT="${GITHUB_PAT}" \
            -e NOTEBOOK="${NOTEBOOK}" \
            ${{ needs.setup.outputs.neurodesk_image }}
          
          echo "Started container: $CONTAINER_NAME"

      - name: Wait for container readiness
        run: |
          echo "Waiting for Neurodesktop to become ready (up to 5 minutes)..."
          for i in {1..300}; do
            if docker logs "$CONTAINER_NAME" 2>&1 | grep -q "To access the server"; then
              echo "Container ready after ${i} seconds"
              exit 0
            fi
            sleep 1
          done
          echo "::error::Container did not become ready within 5 minutes"
          echo "=== Container logs ==="
          docker logs "$CONTAINER_NAME"
          exit 1

      - name: Verify CVMFS inside container
        run: |
          echo "========== VERIFYING CVMFS IN CONTAINER ==========="
          docker exec "$CONTAINER_NAME" su --preserve-environment jovyan -s /bin/bash -c '
            echo "Checking CVMFS mount..."
            if [ -d /cvmfs/neurodesk.ardc.edu.au ]; then
              echo "✓ /cvmfs/neurodesk.ardc.edu.au accessible"
              echo ""
              echo "Contents:"
              ls -la /cvmfs/neurodesk.ardc.edu.au/ || echo "Cannot list!"
              echo ""
              echo "Module system check:"
              if [ -f /usr/share/lmod/lmod/init/bash ]; then
                source /usr/share/lmod/lmod/init/bash
                export MODULEPATH=/cvmfs/neurodesk.ardc.edu.au/containers/modules
                echo "✓ LMOD initialized"
                echo "✓ MODULEPATH=$MODULEPATH"
                echo ""
                echo "Available modules:"
                module avail 2>&1 || echo "No modules found"
              else
                echo "✗ LMOD not found"
              fi
            else
              echo "::error::/cvmfs/neurodesk.ardc.edu.au NOT accessible in container!"
              echo "Container mounts:"
              mount | grep cvmfs || echo "No CVMFS mounts visible"
              exit 1
            fi
          '
          echo "==================================================="

      - name: Setup container environment (as root)
        run: |
          docker exec --user root "$CONTAINER_NAME" bash -c '
            # Remap jovyan UID/GID to match runner user
            if [ -n "$NB_UID" ] && [ "$NB_UID" != "$(id -u jovyan)" ]; then
                echo "Remapping jovyan UID to $NB_UID..."
                usermod -u $NB_UID jovyan
                # Fix ownership of files that were owned by 1000
                find /home/jovyan -user 1000 -exec chown -h $NB_UID {} \; 2>/dev/null || true
            fi
            if [ -n "$NB_GID" ] && [ "$NB_GID" != "$(id -g jovyan)" ]; then
                echo "Remapping users group GID to $NB_GID..."
                groupmod -g $NB_GID users
            fi

            # Create apptainer overlay for modules that require it
            if [ ! -L /tmp/apptainer_overlay ]; then
              echo "Creating /tmp/apptainer_overlay symlink to /overlay-storage..."
              
              # Check if overlay exists in mounted storage
              if [ ! -f /overlay-storage/apptainer_overlay ]; then
                  echo "Creating 300MB overlay file on secondary disk..."
                  dd if=/dev/zero of=/overlay-storage/apptainer_overlay bs=1M count=300 status=none
                  mkfs.ext3 -F /overlay-storage/apptainer_overlay >/dev/null
                  chmod 777 /overlay-storage/apptainer_overlay
              fi
              
              # Symlink to expected location
              ln -sf /overlay-storage/apptainer_overlay /tmp/apptainer_overlay
            fi

            # Move apptainer working dir to secondary storage to save root disk space
            mkdir -p /overlay-storage/var_lib_apptainer/mnt/session
            chmod -R 777 /overlay-storage/var_lib_apptainer
            
            # Symlink /var/lib/apptainer if it doesnt exist or isnt a link yet
            if [ ! -L /var/lib/apptainer ]; then
                rm -rf /var/lib/apptainer # Remove if it exists as a dir
                ln -sf /overlay-storage/var_lib_apptainer /var/lib/apptainer
            fi
            
            # Fix pip cache permissions for jovyan
            if [ -d /home/jovyan/.cache/pip ]; then
                chown -R "$(id -u jovyan):$(id -g jovyan)" /home/jovyan/.cache/pip
            fi
            
            echo "Root-level setup complete"
          '

      - name: Execute notebook in container
        timeout-minutes: 1500
        run: |
          NOTEBOOK_PATH="${{ matrix.notebooks }}"
          NOTEBOOK=$(basename -s .ipynb "$NOTEBOOK_PATH")
          docker exec -e NOTEBOOK_PATH="$NOTEBOOK_PATH" -e NOTEBOOK="$NOTEBOOK" "$CONTAINER_NAME" su --preserve-environment jovyan -s /bin/bash -c '
            set -e
            
            export PATH=/opt/conda/bin:$HOME/.local/bin:$PATH

            # Initialize the module system
            if [ -f /usr/share/lmod/lmod/init/bash ]; then
              source /usr/share/lmod/lmod/init/bash
            elif [ -f /usr/share/module.sh ]; then
              source /usr/share/module.sh
            fi

            # Set MODULEPATH to neurodesk modules
            export MODULEPATH=/cvmfs/neurodesk.ardc.edu.au/containers/modules
            echo "MODULEPATH set to: $MODULEPATH"

            # Install dependencies only if missing (use pip --user)
            pip show nbconvert >/dev/null 2>&1 || pip install --user nbconvert nbclient ipykernel

            # Pre-execution verification
            echo "=== Container Environment ==="
            which jupyter && jupyter --version
            echo "Current directory: $(pwd)"
            echo "Notebook to execute: $NOTEBOOK_PATH"
            echo "Notebook name: $NOTEBOOK"

            echo "Executing notebook: $NOTEBOOK_PATH"
            
            # Execute the notebook in-place using nbconvert
            # --allow-errors: continue even if cells error (we detect errors later)
            # --inplace: overwrite the original notebook with outputs
            # Log is saved to mounted workspace for persistence
            LOG_FILE="$HOME/workspace/nbconvert-${NOTEBOOK}.log"
            
            # CRITICAL: Modify the kernel.json to include MODULEPATH and LMOD_CMD
            # IPython startup scripts are not reliable - kernel may not load them
            # Modifying the kernelspec ensures the kernel process starts with correct environment
            KERNEL_JSON="/opt/conda/share/jupyter/kernels/python3/kernel.json"
            echo "Modifying kernel spec at $KERNEL_JSON"
            cp "$KERNEL_JSON" "${KERNEL_JSON}.bak"
            echo "import json" > /tmp/fix_kernel.py
            echo "k = json.load(open(\"$KERNEL_JSON\"))" >> /tmp/fix_kernel.py
            echo "k.setdefault(\"env\", {}).update({\"MODULEPATH\": \"/cvmfs/neurodesk.ardc.edu.au/containers/modules\", \"LMOD_CMD\": \"/usr/share/lmod/lmod/libexec/lmod\"})" >> /tmp/fix_kernel.py
            echo "json.dump(k, open(\"$KERNEL_JSON\", \"w\"), indent=2)" >> /tmp/fix_kernel.py
            echo "print(\"Updated kernel.json\")" >> /tmp/fix_kernel.py
            python3 /tmp/fix_kernel.py
            cat "$KERNEL_JSON"
            
            # Run jupyter nbconvert
            jupyter nbconvert --to notebook --execute --inplace --allow-errors \
              --ExecutePreprocessor.timeout=3600 \
              --ExecutePreprocessor.kernel_name=python3 \
              --debug \
              "/home/jovyan/workspace/books/$NOTEBOOK_PATH" 2>&1 | tee "$LOG_FILE"
            
            echo "Notebook execution complete"
            
            # Post-execution verification
            echo "=== Post-execution check ==="
            if [ -f "$NOTEBOOK_PATH" ]; then
              ls -la "$NOTEBOOK_PATH"
            else
              echo "::error::Notebook file not found after execution: $NOTEBOOK_PATH"
              exit 1
            fi
          '

      - name: Cleanup container
        if: always()
        run: |
          echo "Stopping and removing container: $CONTAINER_NAME"
          docker stop "$CONTAINER_NAME" 2>/dev/null || true
          docker rm "$CONTAINER_NAME" 2>/dev/null || true

      - name: Check for execution errors
        run: |
          # Check the notebook for errors - step/job fails (red) if errors found, but publish still runs
          python3 << 'EOF'
          import json
          import sys
          import os
          
          notebook_path = "books/${{ matrix.notebooks }}"
          notebook_name = "${{ matrix.notebooks }}"
          errors_found = []
          
          try:
              with open(notebook_path, 'r') as f:
                  nb = json.load(f)
              
              for cell_idx, cell in enumerate(nb.get('cells', [])):
                  if cell.get('cell_type') == 'code':
                      for output in cell.get('outputs', []):
                          if output.get('output_type') == 'error':
                              ename = output.get('ename', 'Unknown')
                              evalue = output.get('evalue', '')
                              errors_found.append({
                                  'cell': cell_idx,
                                  'ename': ename,
                                  'evalue': evalue[:200]  # Truncate long error messages
                              })
              
              if errors_found:
                  # Report as error so the step shows as failed
                  print(f"::error::Notebook {notebook_name} has {len(errors_found)} execution error(s). Please fix the notebook.")
                  
                  # Write to job summary for visibility
                  summary = f"""
          ## ⚠️ Notebook Execution Errors
          
          **Notebook:** `{notebook_name}`
          
          The following errors occurred during execution. The notebook has been published with error outputs visible, but the author should fix these issues:
          
          | Cell | Error Type | Message |
          |------|-----------|---------|
          """
                  for err in errors_found:
                      summary += f"| {err['cell']} | `{err['ename']}` | {err['evalue'][:100]} |\n"
                  
                  summary += "\n\nPlease update the notebook to fix these errors.\n"
                  
                  # Write to GitHub step summary
                  with open(os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null'), 'a') as f:
                      f.write(summary)
                  
                  # Exit with error so step shows as failed (continue-on-error allows workflow to proceed)
                  sys.exit(1)
              else:
                  print("No errors found in notebook outputs")
                  
          except Exception as e:
              print(f"::warning::Could not check notebook for errors: {e}")
          EOF

      - name: Upload executed notebook artifact
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f
        with:
          name: executed-${{ env.NOTEBOOK }}
          path: books/${{ matrix.notebooks }}
          retention-days: 1

  # ---------------------------------------------------------------------------
  # PUBLISH: Build complete book and deploy to GitHub Pages
  # ---------------------------------------------------------------------------
  publish-pages:
    name: Build & Publish to GitHub Pages
    needs: [run-notebooks, setup]
    if: ${{ always() && (needs.run-notebooks.result == 'success' || needs.run-notebooks.result == 'skipped' || needs.run-notebooks.result == 'failure') }}
    runs-on: ubuntu-22.04

    steps:
      - name: Fix workspace permissions (for self-hosted runners)
        if: always()
        run: |
          # Fix any root-owned files from previous Docker runs
          sudo chown -R $USER:$USER ${{ github.workspace }} 2>/dev/null || true
          # Fix git-annex read-only files (annex sets -r--r--r-- to protect data)
          sudo chmod -R u+w ${{ github.workspace }}/.git/annex 2>/dev/null || true
          find ${{ github.workspace }} -name '.git' -type d -exec sudo chmod -R u+w {}/annex \; 2>/dev/null || true

      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5

      - name: Configure runner (Optimize Disk Space)
        if: runner.os == 'Linux'
        run: |
          if [ "${{ needs.setup.outputs.runner }}" != "self-hosted" ]; then
             echo "Optimizing GitHub-hosted runner..."
             sudo swapoff -a && sudo rm -rf /mnt/*
             BASE_PATH=/mnt
          else
             echo "Configuring self-hosted runner..."
             # Be careful not to wipe /mnt on self-hosted if it's used
             BASE_PATH=/storage
          fi
          
          echo "Using BASE_PATH=$BASE_PATH"
          echo "BASE_PATH=$BASE_PATH" >> $GITHUB_ENV
          
          # Setup directories
          sudo mkdir -p $BASE_PATH/tmp
          sudo chown $USER $BASE_PATH/tmp
          
          # Configure Docker data-root (only for GitHub-hosted runners)
          if [ "${{ needs.setup.outputs.runner }}" != "self-hosted" ]; then
            sudo mkdir -p $BASE_PATH/docker
            echo "{\"data-root\": \"$BASE_PATH/docker\"}" | sudo tee /etc/docker/daemon.json
            sudo systemctl restart docker
          fi
          
      - name: Ensure Neurodesk image available
        run: |
          IMAGE="${{ needs.setup.outputs.neurodesk_image }}"
          if docker image inspect "$IMAGE" >/dev/null 2>&1; then
            echo "✓ Image already exists locally"
          else
            echo "Pulling $IMAGE..."
            docker pull "$IMAGE"
          fi

      - name: Restore Conda/Python dependencies cache
        uses: actions/cache@v5.0.1
        with:
          path: ${{ github.workspace }}/.cache/pip
          key: pip-book-${{ hashFiles('books/_config.yml') }}
          restore-keys: |
            pip-book-

      - name: Download all executed notebook artifacts
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131
        with:
          pattern: executed-*
          path: loaded-artifacts

      - name: Merge executed notebooks into books directory
        run: |
          echo "========== MERGING EXECUTED NOTEBOOKS =========="
          
          echo "=== Step 1: Check artifact directory ==="
          if [ -d "loaded-artifacts" ]; then
            echo "loaded-artifacts directory exists"
            ls -la loaded-artifacts/
          else
            echo "::error::loaded-artifacts directory not found!"
            exit 1
          fi
          
          echo ""
          echo "=== Step 2: Find notebook files ==="
          echo "Searching for .ipynb files in loaded-artifacts..."
          find loaded-artifacts -name "*.ipynb" -type f
          
          echo ""
          echo "=== Step 3: Merge notebooks ==="
          merged=0
          
          # Find all .ipynb files in loaded-artifacts (handles both direct files and subdirectories)
          while IFS= read -r nb_file; do
            if [ -f "$nb_file" ]; then
              nb_name=$(basename "$nb_file")
              echo "Processing: $nb_file"
              echo "  Notebook: $nb_name ($(du -h "$nb_file" | cut -f1))"
              
              # Find target in books/
              target=$(find books -name "$nb_name" -type f | head -1)
              
              if [ -n "$target" ]; then
                echo "  Target: $target"
                echo "  Before: $(du -h "$target" | cut -f1)"
                cp -v "$nb_file" "$target"
                echo "  After: $(du -h "$target" | cut -f1)"
                merged=$((merged + 1))
              else
                echo "  ::warning::No target found for $nb_name in books/"
              fi
              echo ""
            fi
          done < <(find loaded-artifacts -name "*.ipynb" -type f)
          
          echo ""
          echo "=== Step 4: Verify merge ==="
          echo "Notebooks merged: $merged"
          if [ $merged -eq 0 ]; then
            echo "::warning::No notebooks were merged!"
          fi
          echo ""
          echo "Recently modified .ipynb files in books/:"
          find books -name "*.ipynb" -type f -mmin -5 -exec ls -lh {} \; 2>/dev/null | head -10 || true
          
          echo "================================================"

      - name: Set container name
        run: echo "CONTAINER_NAME=neurodesktop-publish-${{ github.run_id }}" >> $GITHUB_ENV

      - name: Start Neurodesktop container
        env:
          GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Ensure pip cache directory exists
          mkdir -p ${{ github.workspace }}/.cache/pip

          # -----------------------------------------------------------------------
          # NOTE: We use the exact same Docker startup settings as run-notebooks
          # to ensure the environment (Lmod, Apptainer overlay) initializes
          # correctly without permission errors.
          # -----------------------------------------------------------------------
          sudo docker run -d --name "$CONTAINER_NAME" \
            --shm-size=1gb \
            --privileged \
            --user=root \
            -v ${{ github.workspace }}:/home/jovyan/workspace \
            -v ${{ env.BASE_PATH }}/tmp:/overlay-storage \
            -v ${{ github.workspace }}/.cache/pip:/home/jovyan/.cache/pip \
            -w /home/jovyan/workspace/books \
            -e NB_UID="$(id -u)" \
            -e NB_GID="$(id -g)" \
            -e GITHUB_PAT="${GITHUB_PAT}" \
            ${{ needs.setup.outputs.neurodesk_image }}
          
          echo "Started container: $CONTAINER_NAME"

      - name: Wait for container readiness
        run: |
          echo "Waiting for Neurodesktop to become ready (up to 5 minutes)..."
          for i in {1..300}; do
            if docker logs "$CONTAINER_NAME" 2>&1 | grep -q "To access the server"; then
              echo "Container ready after ${i} seconds"
              exit 0
            fi
            sleep 1
          done
          echo "::error::Container did not become ready within 5 minutes"
          echo "=== Container logs ==="
          docker logs "$CONTAINER_NAME"
          exit 1

      - name: Setup container environment (as root)
        run: |
          docker exec --user root "$CONTAINER_NAME" bash -c '
            # Remap jovyan UID/GID to match runner user
            if [ -n "$NB_UID" ] && [ "$NB_UID" != "$(id -u jovyan)" ]; then
                echo "Remapping jovyan UID to $NB_UID..."
                usermod -u $NB_UID jovyan
                find /home/jovyan -user 1000 -exec chown -h $NB_UID {} \; 2>/dev/null || true
            fi
            if [ -n "$NB_GID" ] && [ "$NB_GID" != "$(id -g jovyan)" ]; then
                echo "Remapping users group GID to $NB_GID..."
                groupmod -g $NB_GID users
            fi
            
            # Fix pip cache permissions
            if [ -d /home/jovyan/.cache/pip ]; then
                chown -R "$(id -u jovyan):$(id -g jovyan)" /home/jovyan/.cache/pip
            fi
            
            echo "Root-level setup complete"
          '

      - name: Build Jupyter Book in container
        run: |
          docker exec "$CONTAINER_NAME" su --preserve-environment jovyan -s /bin/bash -c '
            set -e
            export PATH=/opt/conda/bin:$HOME/.local/bin:$PATH

            # Install dependencies
            # We use pip --user to take advantage of the mounted .cache/pip
            pip install --user "jupyter-book<2.0.0" ghp-import

            # Generate table of contents
            /bin/bash ../.github/scripts/write-toc-entry.sh

            # Disable notebook execution (already executed in run-notebooks job)
            sed -i "s/execute_notebooks: .*/execute_notebooks: off/" _config.yml
            
            # Build the book
            jupyter-book build .
          '

      - name: Cleanup container
        if: always()
        run: |
          echo "Stopping and removing container: $CONTAINER_NAME"
          docker stop "$CONTAINER_NAME" 2>/dev/null || true
          docker rm "$CONTAINER_NAME" 2>/dev/null || true

      - name: Remove large files to avoid GitHub size limits
        run: |
          # _sources contains copies of executed notebooks which can exceed GitHub's 100MB limit
          echo "Removing _sources folder from build output..."
          rm -rf books/_build/html/_sources
          echo "✓ _sources folder removed"
          
          # Remove HTML files that exceed GitHub's 100MB limit (temporary fix)
          echo "Removing oversized HTML files..."
          rm -f books/_build/html/examples/diffusion_imaging/MRtrix_3.html
          rm -f books/_build/html/examples/workflows/ipyniivue_ipywidgets.html
          rm -f books/_build/html/examples/structural_imaging/FSL_course_bet.html
          echo "✓ Oversized HTML files removed"

      - name: Verify build output
        run: |
          echo "========== VERIFYING BUILD OUTPUT =========="
          
          if [ -d "books/_build/html" ]; then
            echo "✓ books/_build/html exists"
            echo ""
            echo "Build output size:"
            du -sh books/_build/html
            echo ""
            echo "Top-level files:"
            ls -lh books/_build/html/ | head -20
            echo ""
            echo "Recently modified HTML files:"
            find books/_build/html -name "*.html" -type f -mmin -10 2>/dev/null | head -10 || true
          else
            echo "::error::books/_build/html directory not found!"
            echo "Listing books/_build:"
            ls -la books/_build/ || echo "books/_build does not exist"
            exit 1
          fi
          
          echo "============================================"

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@4f9cc6602d3f66b9c108549d475ec49e8ef4d45e
        if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./books/_build/html
          keep_files: true
